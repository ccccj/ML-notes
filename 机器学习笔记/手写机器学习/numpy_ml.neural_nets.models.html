
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Full networks &#8212; numpy-ml 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Utilities" href="numpy_ml.neural_nets.utils.html" />
    <link rel="prev" title="Modules" href="numpy_ml.neural_nets.modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">numpy-ml</a></h1>



<p class="blurb">Machine learning, in NumPy</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=ddbourgin&repo=numpy-ml&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.hmm.html">Hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.gmm.html">Gaussian mixture models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.lda.html">Latent Dirichlet allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.ngram.html">N-gram smoothing models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.rl_models.html">Reinforcement learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.nonparametric.html">Nonparametric models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.trees.html">Tree-based models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="numpy_ml.neural_nets.html">Neural networks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.layers.html">Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.losses.html">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.optimizers.html">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.schedulers.html">Learning rate schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.wrappers.html">Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.modules.html">Modules</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Full networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#wgan-gp"><code class="docutils literal notranslate"><span class="pre">WGAN_GP</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#bernoullivae"><code class="docutils literal notranslate"><span class="pre">BernoulliVAE</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#word2vec"><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.utils.html">Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.linear_models.html">Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="numpy_ml.neural_nets.html">Neural networks</a><ul>
      <li>Previous: <a href="numpy_ml.neural_nets.modules.html" title="previous chapter">Modules</a></li>
      <li>Next: <a href="numpy_ml.neural_nets.utils.html" title="next chapter">Utilities</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="full-networks">
<h1>Full networks<a class="headerlink" href="#full-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="wgan-gp">
<h2><code class="docutils literal notranslate"><span class="pre">WGAN_GP</span></code><a class="headerlink" href="#wgan-gp" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.models.WGAN_GP">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.models.</code><code class="descname">WGAN_GP</code><span class="sig-paren">(</span><em>g_hidden=512</em>, <em>init='he_uniform'</em>, <em>optimizer='RMSProp(lr=0.0001)'</em>, <em>debug=False</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L11-L528"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP" title="Permalink to this definition">¶</a></dt>
<dd><p>A Wasserstein generative adversarial network (WGAN) architecture with
gradient penalty (GP).</p>
<p class="rubric">Notes</p>
<p>In contrast to a regular WGAN, WGAN-GP uses gradient penalty on the
generator rather than weight clipping to encourage the 1-Lipschitz
constraint:</p>
<div class="math notranslate nohighlight">
\[| \text{Generator}(\mathbf{x}_1) - \text{Generator}(\mathbf{x}_2) |
    \leq |\mathbf{x}_1 - \mathbf{x}_2 | \ \ \ \ \forall \mathbf{x}_1, \mathbf{x}_2\]</div>
<p>In other words, the generator must have input gradients with a norm of at
most 1 under the <span class="math notranslate nohighlight">\(\mathbf{X}_{real}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}_{fake}\)</span>
data distributions.</p>
<p>To enforce this constraint, WGAN-GP penalizes the model if the generator
gradient norm moves away from a target norm of 1. See
<a class="reference internal" href="numpy_ml.neural_nets.losses.html#numpy_ml.neural_nets.losses.WGAN_GPLoss" title="numpy_ml.neural_nets.losses.WGAN_GPLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">WGAN_GPLoss</span></code></a> for more details.</p>
<p>In contrast to a standard WGAN, WGAN-GP avoids using BatchNorm in the
critic, as correlation between samples in a batch can impact the stability
of the gradient penalty.</p>
<p>WGAP-GP architecture:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>X_real ------------------------|
                                &gt;---&gt; [Critic] --&gt; Y_out
Z --&gt; [Generator] --&gt; X_fake --|
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">[Generator]</span></code> is</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FC1 -&gt; ReLU -&gt; FC2 -&gt; ReLU -&gt; FC3 -&gt; ReLU -&gt; FC4
</pre></div>
</div>
<p>and <code class="docutils literal notranslate"><span class="pre">[Critic]</span></code> is</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FC1 -&gt; ReLU -&gt; FC2 -&gt; ReLU -&gt; FC3 -&gt; ReLU -&gt; FC4
</pre></div>
</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[Z \sim \mathcal{N}(0, 1)\]</div>
<p>Wasserstein generative adversarial network with gradient penalty.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>g_hidden</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of units in the critic and generator hidden layers.
Default is 512.</li>
<li><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The weight initialization strategy. Valid entries are
{‘glorot_normal’, ‘glorot_uniform’, ‘he_normal’, ‘he_uniform’,
‘std_normal’, ‘trunc_normal’}. Default is “he_uniform”.</li>
<li><strong>optimizer</strong> (str or <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object or None) – The optimization strategy to use when performing gradient updates.
If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a>
optimizer with default parameters. Default is “RMSProp(lr=0.0001)”.</li>
<li><strong>debug</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to store additional intermediate output within
<code class="docutils literal notranslate"><span class="pre">self.derived_variables</span></code>. Default is False.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L153-L167"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L169-L176"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L178-L189"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L191-L204"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>module</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L206-L244"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform the forward pass for either the generator or the critic.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(batchsize, *)</cite>) – Input data</li>
<li><strong>module</strong> (<em>{'C'</em><em> or </em><em>'G'}</em>) – Whether to perform the forward pass for the critic (‘C’) or for the
generator (‘G’).</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>out</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(batchsize, *)</cite>) – The output of the final layer of the module.</li>
<li><strong>Xs</strong> (<em>dict</em>) – A dictionary with layer ids as keys and values corresponding to the
input to each intermediate layer during the forward pass. Useful
during debugging.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>grad</em>, <em>module</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L246-L282"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform the backward pass for either the generator or the critic.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>grad</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(batchsize, *)</cite> or list of arrays) – Gradient of the loss with respect to module output(s).</li>
<li><strong>module</strong> (<em>{'C'</em><em> or </em><em>'G'}</em>) – Whether to perform the backward pass for the critic (‘C’) or for the
generator (‘G’).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>out</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(batchsize, *)</cite>) – The gradient of the loss with respect to the module input.</li>
<li><strong>dXs</strong> (<em>dict</em>) – A dictionary with layer ids as keys and values corresponding to the
input to each intermediate layer during the backward pass. Useful
during debugging.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.update_critic">
<code class="descname">update_critic</code><span class="sig-paren">(</span><em>X_real</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L304-L393"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.update_critic" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute parameter gradients for the critic on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X_real</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(batchsize, n_feats)</cite>) – Input data.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>C_loss</strong> (<em>float</em>) – The critic loss on the current data.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.update_generator">
<code class="descname">update_generator</code><span class="sig-paren">(</span><em>X_shape</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L395-L422"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.update_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute parameter gradients for the generator on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X_shape</strong> (tuple of <cite>(batchsize, n_feats)</cite>) – Shape for the input batch.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>G_loss</strong> (<em>float</em>) – The generator loss on the fake data (generated during the critic
update)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L424-L434"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset parameter gradients to 0 after an update.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>module</em>, <em>module_loss=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L436-L447"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient updates and flush gradients upon completion</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.WGAN_GP.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X_real</em>, <em>lambda_</em>, <em>n_steps=1000</em>, <em>batchsize=128</em>, <em>c_updates_per_epoch=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/wgan_gp.py#L449-L528"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.WGAN_GP.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit WGAN_GP on a training dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X_real</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_feats)</cite>) – Training dataset</li>
<li><strong>lambda</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Gradient penalty coefficient for the critic loss</li>
<li><strong>n_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The maximum number of generator updates to perform. Default is
1000.</li>
<li><strong>batchsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of examples to use in each training minibatch. Default is
128.</li>
<li><strong>c_updates_per_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of critic updates to perform at each generator update.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Print loss values after each update. If False, only print loss
every 100 steps. Default is True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bernoullivae">
<h2><code class="docutils literal notranslate"><span class="pre">BernoulliVAE</span></code><a class="headerlink" href="#bernoullivae" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.models.</code><code class="descname">BernoulliVAE</code><span class="sig-paren">(</span><em>T=5</em>, <em>latent_dim=256</em>, <em>enc_conv1_pad=0</em>, <em>enc_conv2_pad=0</em>, <em>enc_conv1_out_ch=32</em>, <em>enc_conv2_out_ch=64</em>, <em>enc_conv1_stride=1</em>, <em>enc_pool1_stride=2</em>, <em>enc_conv2_stride=1</em>, <em>enc_pool2_stride=1</em>, <em>enc_conv1_kernel_shape=(5</em>, <em>5)</em>, <em>enc_pool1_kernel_shape=(2</em>, <em>2)</em>, <em>enc_conv2_kernel_shape=(5</em>, <em>5)</em>, <em>enc_pool2_kernel_shape=(2</em>, <em>2)</em>, <em>optimizer='RMSProp(lr=0.0001)'</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L12-L453"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE" title="Permalink to this definition">¶</a></dt>
<dd><p>A variational autoencoder (VAE) with 2D convolutional encoder and Bernoulli
input and output units.</p>
<p class="rubric">Notes</p>
<p>The VAE architecture is</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>                |-- t_mean ----|
X -&gt; [Encoder] -|              |--&gt; [Sampler] -&gt; [Decoder] -&gt; X_recon
                |-- t_log_var -|
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">[Encoder]</span></code> is</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Conv1 -&gt; ReLU -&gt; MaxPool1 -&gt; Conv2 -&gt; ReLU -&gt;
    MaxPool2 -&gt; Flatten -&gt; FC1 -&gt; ReLU -&gt; FC2
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">[Decoder]</span></code> is</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FC1 -&gt; FC2 -&gt; Sigmoid
</pre></div>
</div>
<p>and <code class="docutils literal notranslate"><span class="pre">[Sampler]</span></code> draws a sample from the distribution</p>
<div class="math notranslate nohighlight">
\[\mathcal{N}(\text{t_mean}, \exp \left\{\text{t_log_var}\right\} I)\]</div>
<p>using the reparameterization trick.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>T</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the variational parameter <cite>t</cite>. Default is 5.</li>
<li><strong>enc_conv1_pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The padding for the first convolutional layer of the encoder. Default is 0.</li>
<li><strong>enc_conv1_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride for the first convolutional layer of the encoder. Default is 1.</li>
<li><strong>enc_conv1_out_ch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channels for the first convolutional layer of
the encoder. Default is 32.</li>
<li><strong>enc_conv1_kernel_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The number of rows and columns in each filter of the first
convolutional layer of the encoder. Default is (5, 5).</li>
<li><strong>enc_pool1_kernel_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The number of rows and columns in the receptive field of the first
max pool layer of the encoder. Default is (2, 3).</li>
<li><strong>enc_pool1_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride for the first MaxPool layer of the encoder. Default is
2.</li>
<li><strong>enc_conv2_pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The padding for the second convolutional layer of the encoder.
Default is 0.</li>
<li><strong>enc_conv2_out_ch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channels for the second convolutional layer of
the encoder. Default is 64.</li>
<li><strong>enc_conv2_kernel_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The number of rows and columns in each filter of the second
convolutional layer of the encoder. Default is (5, 5).</li>
<li><strong>enc_conv2_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride for the second convolutional layer of the encoder.
Default is 1.</li>
<li><strong>enc_pool2_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride for the second MaxPool layer of the encoder. Default is
1.</li>
<li><strong>enc_pool2_kernel_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The number of rows and columns in the receptive field of the second
max pool layer of the encoder. Default is (2, 3).</li>
<li><strong>latent_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the output for the first FC layer of the encoder.
Default is 256.</li>
<li><strong>optimizer</strong> (str or <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object or None) – The optimization strategy to use when performing gradient updates.
If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a>
optimizer with default parameters. Default is “RMSProp(lr=0.0001)”.</li>
<li><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The weight initialization strategy. Valid entries are
{‘glorot_normal’, ‘glorot_uniform’, ‘he_normal’, ‘he_uniform’,
‘std_normal’, ‘trunc_normal’}. Default is ‘glorot_uniform’.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L210-L217"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L219-L247"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L249-L272"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L274-L281"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X_train</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L308-L339"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>VAE forward pass</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>X_train</em>, <em>X_recon</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L341-L385"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>VAE backward pass</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>cur_loss=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L387-L393"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient updates</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L395-L400"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset parameter gradients after update</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.BernoulliVAE.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X_train</em>, <em>n_epochs=20</em>, <em>batchsize=128</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/vae.py#L402-L453"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.BernoulliVAE.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the VAE to a training dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X_train</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The input volume</li>
<li><strong>n_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The maximum number of training epochs to run. Default is 20.</li>
<li><strong>batchsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The desired number of examples in each training batch. Default is 128.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Print batch information during training. Default is True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="word2vec">
<h2><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code><a class="headerlink" href="#word2vec" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.models.Word2Vec">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.models.</code><code class="descname">Word2Vec</code><span class="sig-paren">(</span><em>context_len=5</em>, <em>min_count=None</em>, <em>skip_gram=False</em>, <em>max_tokens=None</em>, <em>embedding_dim=300</em>, <em>filter_stopwords=True</em>, <em>noise_dist_power=0.75</em>, <em>init='glorot_uniform'</em>, <em>num_negative_samples=64</em>, <em>optimizer='SGD(lr=0.1)'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L12-L451"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec" title="Permalink to this definition">¶</a></dt>
<dd><p>A word2vec model supporting both continuous bag of words (CBOW) and
skip-gram architectures, with training via noise contrastive
estimation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of words to the left and right of the current word to
use as context during training. Larger values result in more
training examples and thus can lead to higher accuracy at the
expense of additional training time. Default is 5.</li>
<li><strong>min_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.8)"><em>None</em></a>) – Minimum number of times a token must occur in order to be included
in vocab. If None, include all tokens from <cite>corpus_fp</cite> in vocab.
Default is None.</li>
<li><strong>skip_gram</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to train the skip-gram or CBOW model. The skip-gram model
is trained to predict the target word i given its surrounding
context, <code class="docutils literal notranslate"><span class="pre">words[i</span> <span class="pre">-</span> <span class="pre">context:i]</span></code> and <code class="docutils literal notranslate"><span class="pre">words[i</span> <span class="pre">+</span> <span class="pre">1:i</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">+</span>
<span class="pre">context]</span></code> as input. Default is False.</li>
<li><strong>max_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.8)"><em>None</em></a>) – Only add the first <cite>max_tokens</cite> most frequent tokens that occur
more than <cite>min_count</cite> to the vocabulary.  If None, add all tokens
that occur more than than <cite>min_count</cite>. Default is None.</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of dimensions in the final word embeddings. Default is
300.</li>
<li><strong>filter_stopwords</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to remove stopwords before encoding the words in the
corpus. Default is True.</li>
<li><strong>noise_dist_power</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The power the unigram count is raised to when computing the noise
distribution for negative sampling. A value of 0 corresponds to a
uniform distribution over tokens, and a value of 1 corresponds to a
distribution proportional to the token unigram counts. Default is
0.75.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is ‘glorot_uniform’.</li>
<li><strong>num_negative_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of negative samples to draw from the noise distribution
for each positive training sample. If 0, use the hierarchical
softmax formulation of the model instead. Default is 5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <cite>update</cite> method.  If None, use the
<a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.parameters" title="numpy_ml.neural_nets.layers.LSTM.parameters"><strong>parameters</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.Add.hyperparameters" title="numpy_ml.neural_nets.layers.Add.hyperparameters"><strong>hyperparameters</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.derived_variables" title="numpy_ml.neural_nets.layers.LSTM.derived_variables"><strong>derived_variables</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.gradients" title="numpy_ml.neural_nets.layers.LSTM.gradients"><strong>gradients</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The word2vec model is outlined in in [1].</p>
<p>CBOW architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="p">}</span>   <span class="o">----|</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span> <span class="o">----|</span>
<span class="o">...</span>            <span class="o">--&gt;</span> <span class="n">Average</span> <span class="o">--&gt;</span> <span class="n">Embedding</span> <span class="n">layer</span> <span class="o">--&gt;</span> <span class="p">[</span><span class="n">NCE</span> <span class="n">Layer</span> <span class="o">/</span> <span class="n">HSoftmax</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="o">...</span><span class="p">})</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">----|</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="p">}</span>   <span class="o">----|</span>
</pre></div>
</div>
<p>Skip-gram architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">--&gt;</span> <span class="n">Embedding</span> <span class="n">layer</span> <span class="o">--&gt;</span> <span class="p">[</span><span class="n">NCE</span> <span class="n">Layer</span> <span class="o">/</span> <span class="n">HSoftmax</span><span class="p">]</span> <span class="o">--|</span>     <span class="o">...</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
</pre></div>
</div>
<p>where <span class="math notranslate nohighlight">\(w_{i}\)</span> is the one-hot representation of the word at position
<cite>i</cite> within a sentence in the corpus and <cite>R</cite> is the length of the context
window on either side of the target word.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Mikolov et al. (2013). “Distributed representations of words
and phrases and their compositionality,” Proceedings of the 26th
International Conference on Neural Information Processing Systems.
<a class="reference external" href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a></td></tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.Word2Vec.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L146-L155"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Model parameters</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.Word2Vec.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L157-L180"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Model hyperparameters</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.Word2Vec.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L182-L193"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Variables computed during model operation</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.Word2Vec.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L195-L204"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Model parameter gradients</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>targets</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L206-L233"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the network on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing a minibatch of <cite>n_ex</cite> examples, each
consisting of <cite>n_in</cite> integer word indices</li>
<li><strong>targets</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex,)</cite>) – Target word index for each example in the minibatch.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If <cite>False</cite>, this suggests the layer
will not be expected to backprop through wrt. this input. Default
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>loss</strong> (<em>float</em>) – The loss associated with the current minibatch</li>
<li><strong>y_pred</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex,)</cite>) – The conditional probabilities of the words in <cite>targets</cite> given the
corresponding example / context in <cite>X</cite>.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L235-L240"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradient of the loss wrt the current network parameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>cur_loss=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L242-L246"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient updates</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L248-L251"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset parameter gradients after update</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.get_embedding">
<code class="descname">get_embedding</code><span class="sig-paren">(</span><em>word_ids</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L253-L269"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.get_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the embeddings for a collection of word IDs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>word_ids</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(M,)</cite>) – An array of word IDs to retrieve embeddings for.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>embeddings</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(M, n_out)</cite>) – The embedding vectors for each of the <cite>M</cite> word IDs.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.minibatcher">
<code class="descname">minibatcher</code><span class="sig-paren">(</span><em>corpus_fps</em>, <em>encoding</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L314-L397"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.minibatcher" title="Permalink to this definition">¶</a></dt>
<dd><p>A minibatch generator for skip-gram and CBOW models.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>corpus_fps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><em>list of strs</em>) – The filepath / list of filepaths to the document(s) to be encoded.
Each document is expected to be encoded as newline-separated
string of text, with adjacent tokens separated by a whitespace
character.</li>
<li><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the text encoding for corpus. This value is passed
directly to Python’s <cite>open</cite> builtin. Common entries are either
‘utf-8’ (no header byte), or ‘utf-8-sig’ (header byte).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> (list of length <cite>batchsize</cite> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, <cite>n_in</cite>)) – The context IDs for a minibatch of <cite>batchsize</cite> examples. If
<code class="docutils literal notranslate"><span class="pre">self.skip_gram</span></code> is False, <cite>X</cite> will be a ragged list consisting
of <cite>batchsize</cite> variable-length lists. If <code class="docutils literal notranslate"><span class="pre">self.skip_gram</span></code> is
<cite>True</cite>, all sublists will be of the same length (<cite>n_in</cite>) and <cite>X</cite>
will be returned as a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, <cite>n_in</cite>).</li>
<li><strong>target</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, 1)) – The target IDs associated with each example in <cite>X</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.Word2Vec.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>corpus_fps</em>, <em>encoding='utf-8-sig'</em>, <em>n_epochs=20</em>, <em>batchsize=128</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L399-L451"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.Word2Vec.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn word2vec embeddings for the examples in <cite>X_train</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>corpus_fps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><em>list of strs</em>) – The filepath / list of filepaths to the document(s) to be encoded.
Each document is expected to be encoded as newline-separated
string of text, with adjacent tokens separated by a whitespace
character.</li>
<li><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the text encoding for corpus. Common entries are either
‘utf-8’ (no header byte), or ‘utf-8-sig’ (header byte).  Default
value is ‘utf-8-sig’.</li>
<li><strong>n_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The maximum number of training epochs to run. Default is 20.</li>
<li><strong>batchsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The desired number of examples in each training batch. Default is
128.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Print batch information during training. Default is True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2019, David Bourgin.
      
      |
      <a href="_sources/numpy_ml.neural_nets.models.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-65839510-3']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>