
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Layers &#8212; numpy-ml 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Activations" href="numpy_ml.neural_nets.activations.html" />
    <link rel="prev" title="Neural networks" href="numpy_ml.neural_nets.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">numpy-ml</a></h1>



<p class="blurb">Machine learning, in NumPy</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=ddbourgin&repo=numpy-ml&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.hmm.html">Hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.gmm.html">Gaussian mixture models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.lda.html">Latent Dirichlet allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.ngram.html">N-gram smoothing models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.rl_models.html">Reinforcement learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.nonparametric.html">Nonparametric models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.trees.html">Tree-based models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="numpy_ml.neural_nets.html">Neural networks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#layerbase"><code class="docutils literal notranslate"><span class="pre">LayerBase</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#add"><code class="docutils literal notranslate"><span class="pre">Add</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm1d"><code class="docutils literal notranslate"><span class="pre">BatchNorm1D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchnorm2d"><code class="docutils literal notranslate"><span class="pre">BatchNorm2D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv1d"><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#conv2d"><code class="docutils literal notranslate"><span class="pre">Conv2D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#deconv2d"><code class="docutils literal notranslate"><span class="pre">Deconv2D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#dotproductattention"><code class="docutils literal notranslate"><span class="pre">DotProductAttention</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding"><code class="docutils literal notranslate"><span class="pre">Embedding</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#fullyconnected"><code class="docutils literal notranslate"><span class="pre">FullyConnected</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstm"><code class="docutils literal notranslate"><span class="pre">LSTM</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#lstmcell"><code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#layernorm1d"><code class="docutils literal notranslate"><span class="pre">LayerNorm1D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#layernorm2d"><code class="docutils literal notranslate"><span class="pre">LayerNorm2D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiply"><code class="docutils literal notranslate"><span class="pre">Multiply</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pool2d"><code class="docutils literal notranslate"><span class="pre">Pool2D</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnn"><code class="docutils literal notranslate"><span class="pre">RNN</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnncell"><code class="docutils literal notranslate"><span class="pre">RNNCell</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#rbm"><code class="docutils literal notranslate"><span class="pre">RBM</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#sparseevolution"><code class="docutils literal notranslate"><span class="pre">SparseEvolution</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.losses.html">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.optimizers.html">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.schedulers.html">Learning rate schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.wrappers.html">Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.models.html">Full networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy_ml.neural_nets.utils.html">Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.linear_models.html">Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="numpy_ml.neural_nets.html">Neural networks</a><ul>
      <li>Previous: <a href="numpy_ml.neural_nets.html" title="previous chapter">Neural networks</a></li>
      <li>Next: <a href="numpy_ml.neural_nets.activations.html" title="next chapter">Activations</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="layers">
<h1>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="layerbase">
<h2><code class="docutils literal notranslate"><span class="pre">LayerBase</span></code><a class="headerlink" href="#layerbase" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.layers.</code><code class="descname">LayerBase</code><span class="sig-paren">(</span><em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L26-L133"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="Permalink to this definition">¶</a></dt>
<dd><p>An abstract base class inherited by all nerual network layers</p>
<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>z</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L44-L46"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>out</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L48-L50"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L52-L57"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze the layer parameters at their current values so they can no
longer be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L59-L61"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze the layer parameters so they can be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L63-L71"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Erase all the layer’s derived variables and gradients.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>cur_loss=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L73-L83"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the layer parameters using the accrued gradients and layer
optimizer. Flush all gradients once the update is complete.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>summary_dict</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L85-L125"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the layer parameters from a dictionary of values.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>summary_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – A dictionary of layer parameters and hyperparameters. If a required
parameter or hyperparameter is not included within <cite>summary_dict</cite>,
this method will use the value in the current layer’s
<a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase.summary" title="numpy_ml.neural_nets.layers.layers.LayerBase.summary"><code class="xref py py-meth docutils literal notranslate"><span class="pre">summary()</span></code></a> method.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>layer</strong> (<a class="reference internal" href="#"><span class="doc">Layer</span></a> object) – The newly-initialized layer.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.layers.LayerBase.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L127-L133"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.layers.LayerBase.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dict of the layer parameters, hyperparameters, and ID.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="add">
<h2><code class="docutils literal notranslate"><span class="pre">Add</span></code><a class="headerlink" href="#add" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Add">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Add</code><span class="sig-paren">(</span><em>act_fn=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L592-L685"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Add" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>An “addition” layer that returns the sum of its inputs, passed through
an optional nonlinearity.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The element-wise output nonlinearity used in computing the final
output. If None, use the identity function <span class="math notranslate nohighlight">\(f(x) = x\)</span>.
Default is None.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Add.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L617-L627"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Add.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Add.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L629-L654"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Add.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (list of length <cite>n_inputs</cite>) – A list of tensors, all of the same shape.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *)</cite>) – The sum over the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Add.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdY</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L656-L680"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Add.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (list of length <cite>n_inputs</cite>) – The gradient of the loss wrt. each input in <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="batchnorm1d">
<h2><code class="docutils literal notranslate"><span class="pre">BatchNorm1D</span></code><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.BatchNorm1D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">BatchNorm1D</code><span class="sig-paren">(</span><em>momentum=0.9</em>, <em>epsilon=1e-05</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1118-L1327"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A batch normalization layer for 1D inputs.</p>
<p class="rubric">Notes</p>
<p>BatchNorm is an attempt address the problem of internal covariate
shift (ICS) during training by normalizing layer inputs.</p>
<p>ICS refers to the change in the distribution of layer inputs during
training as a result of the changing parameters of the previous
layer(s). ICS can make it difficult to train models with saturating
nonlinearities, and in general can slow training by requiring a lower
learning rate.</p>
<p>Equations [train]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Equations [test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>In contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.LayerNorm1D" title="numpy_ml.neural_nets.layers.LayerNorm1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm1D</span></code></a>, the BatchNorm layer calculates
the mean and var across the <em>batch</em> rather than the output features.
This has two disadvantages:</p>
<blockquote>
<div><p>1. It is highly affected by batch size: smaller mini-batch sizes
increase the variance of the estimates for the global mean and
variance.</p>
<p>2. It is difficult to apply in RNNs – one must fit a separate
BatchNorm layer for <em>each</em> time-step.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The momentum term for the running mean/running std calculations.
The closer this is to 1, the less weight will be given to the
mean/std of the current batch (i.e., higher smoothing). Default is
0.9.</li>
<li><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small smoothing constant to use during computation of <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>
to avoid divide-by-zero errors. Default is 1e-5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.BatchNorm1D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1206-L1220"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm1D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm1D.reset_running_stats">
<code class="descname">reset_running_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1222-L1226"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm1D.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the running mean and variance estimates to 0 and 1.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm1D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1228-L1274"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm1D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use the current intput to adjust the running mean and
running_var computations. Setting this to True is the same as
freezing the layer for the current input. Default is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer output for each of the <cite>n_ex</cite> examples</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm1D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1276-L1308"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm1D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h2><code class="docutils literal notranslate"><span class="pre">BatchNorm2D</span></code><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.BatchNorm2D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">BatchNorm2D</code><span class="sig-paren">(</span><em>momentum=0.9</em>, <em>epsilon=1e-05</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L883-L1115"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A batch normalization layer for two-dimensional inputs with an
additional channel dimension.</p>
<p class="rubric">Notes</p>
<p>BatchNorm is an attempt address the problem of internal covariate
shift (ICS) during training by normalizing layer inputs.</p>
<p>ICS refers to the change in the distribution of layer inputs during
training as a result of the changing parameters of the previous
layer(s). ICS can make it difficult to train models with saturating
nonlinearities, and in general can slow training by requiring a lower
learning rate.</p>
<p>Equations [train]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Equations [test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>In contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.LayerNorm2D" title="numpy_ml.neural_nets.layers.LayerNorm2D"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm2D</span></code></a>, the BatchNorm layer calculates
the mean and var across the <em>batch</em> rather than the output features.
This has two disadvantages:</p>
<blockquote>
<div><p>1. It is highly affected by batch size: smaller mini-batch sizes
increase the variance of the estimates for the global mean and
variance.</p>
<p>2. It is difficult to apply in RNNs – one must fit a separate
BatchNorm layer for <em>each</em> time-step.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The momentum term for the running mean/running std calculations.
The closer this is to 1, the less weight will be given to the
mean/std of the current batch (i.e., higher smoothing). Default is
0.9.</li>
<li><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small smoothing constant to use during computation of <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>
to avoid divide-by-zero errors. Default is 1e-5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.BatchNorm2D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L973-L987"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm2D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm2D.reset_running_stats">
<code class="descname">reset_running_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L989-L993"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm2D.reset_running_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the running mean and variance estimates to 0 and 1.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L995-L1056"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<p class="rubric">Notes</p>
<p>Equations [train]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Equations [test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">running_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>In contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.LayerNorm2D" title="numpy_ml.neural_nets.layers.LayerNorm2D"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm2D</span></code></a>, the BatchNorm layer calculates the
mean and var across the <em>batch</em> rather than the output features.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – Input volume containing the <cite>in_rows</cite> x <cite>in_cols</cite>-dimensional
features for a minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use the current intput to adjust the running mean and
running_var computations. Setting this to True is the same as
freezing the layer for the current input. Default is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.BatchNorm2D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1058-L1090"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.BatchNorm2D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="conv1d">
<h2><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Conv1D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Conv1D</code><span class="sig-paren">(</span><em>out_ch</em>, <em>kernel_width</em>, <em>pad=0</em>, <em>stride=1</em>, <em>dilation=0</em>, <em>act_fn=None</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2405-L2680"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>Apply a one-dimensional convolution kernel over an input volume.</p>
<p class="rubric">Notes</p>
<p>Equations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">act_fn</span><span class="p">(</span><span class="n">pad</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">W</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">out_dim</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_rows_in</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_width</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</pre></div>
</div>
<p>where ‘<cite>*</cite>’ denotes the cross-correlation operation with stride <cite>s</cite> and dilation <cite>d</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>out_ch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of filters/kernels to compute in the current layer</li>
<li><strong>kernel_width</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The width of a single 1D filter/kernel in the current layer</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">Y[t]</span></code>. If None, use the
identity function <span class="math notranslate nohighlight">\(f(x) = x\)</span> by default. Default is None.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, or </em><em>{'same'</em><em>, </em><em>'causal'}</em>) – The number of rows/columns to zero-pad the input with. If <cite>‘same’</cite>,
calculate padding to ensure the output length matches in the input
length. If <cite>‘causal’</cite> compute padding such that the output both has
the same length as the input AND <code class="docutils literal notranslate"><span class="pre">output[t]</span></code> does not depend on
<code class="docutils literal notranslate"><span class="pre">input[t</span> <span class="pre">+</span> <span class="pre">1:]</span></code>. Default is 0.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride/hop of the convolution kernels as they move over the
input volume. Default is 1.</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of pixels inserted between kernel elements. Effective kernel
shape after dilation is: <code class="docutils literal notranslate"><span class="pre">[kernel_rows</span> <span class="pre">*</span> <span class="pre">(d</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">d,</span> <span class="pre">kernel_cols</span>
<span class="pre">*</span> <span class="pre">(d</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">d]</span></code>. Default is 0.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Conv1D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2483-L2500"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv1D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Conv1D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2502-L2542"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv1D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output given input volume <cite>X</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, l_in, in_ch)</cite>) – The input volume consisting of <cite>n_ex</cite> examples, each of length
<cite>l_in</cite> and with <cite>in_ch</cite> input channels</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, l_out, out_ch)</cite>) – The layer output.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Conv1D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2544-L2585"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv1D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradient of the loss with respect to the layer parameters.</p>
<p class="rubric">Notes</p>
<p>Relies on <a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.im2col" title="numpy_ml.neural_nets.utils.im2col"><code class="xref py py-meth docutils literal notranslate"><span class="pre">im2col()</span></code></a> and
<a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.col2im" title="numpy_ml.neural_nets.utils.col2im"><code class="xref py py-meth docutils literal notranslate"><span class="pre">col2im()</span></code></a> to vectorize the
gradient calculation.  See the private method <code class="xref py py-meth docutils literal notranslate"><span class="pre">_backward_naive()</span></code>
for a more straightforward implementation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, l_out, out_ch)</cite> or list of arrays) – The gradient(s) of the loss with respect to the layer output(s).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, l_in, in_ch)</cite>) – The gradient of the loss with respect to the layer input volume.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="conv2d">
<h2><code class="docutils literal notranslate"><span class="pre">Conv2D</span></code><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Conv2D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Conv2D</code><span class="sig-paren">(</span><em>out_ch</em>, <em>kernel_shape</em>, <em>pad=0</em>, <em>stride=1</em>, <em>dilation=0</em>, <em>act_fn=None</em>, <em>optimizer=None</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2683-L2953"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>Apply a two-dimensional convolution kernel over an input volume.</p>
<p class="rubric">Notes</p>
<p>Equations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">act_fn</span><span class="p">(</span><span class="n">pad</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">W</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">n_rows_out</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_rows_in</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">filter_rows</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
<span class="n">n_cols_out</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_cols_in</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">filter_cols</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</pre></div>
</div>
<p>where <cite>‘*’</cite> denotes the cross-correlation operation with stride <cite>s</cite> and
dilation <cite>d</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>out_ch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of filters/kernels to compute in the current layer</li>
<li><strong>kernel_shape</strong> (<em>2-tuple</em>) – The dimension of a single 2D filter/kernel in the current layer</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">Y[t]</span></code>. If None, use the
identity function <span class="math notranslate nohighlight">\(f(X) = X\)</span> by default. Default is None.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, or </em><em>'same'</em>) – The number of rows/columns to zero-pad the input with. Default is
0.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride/hop of the convolution kernels as they move over the
input volume. Default is 1.</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of pixels inserted between kernel elements. Effective kernel
shape after dilation is: <code class="docutils literal notranslate"><span class="pre">[kernel_rows</span> <span class="pre">*</span> <span class="pre">(d</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">d,</span> <span class="pre">kernel_cols</span>
<span class="pre">*</span> <span class="pre">(d</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">d]</span></code>. Default is 0.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Conv2D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2761-L2778"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv2D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Conv2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2780-L2820"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output given input volume <cite>X</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The input volume consisting of <cite>n_ex</cite> examples, each with dimension
(<cite>in_rows</cite>, <cite>in_cols</cite>, <cite>in_ch</cite>).</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, out_rows, out_cols, out_ch)</cite>) – The layer output.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Conv2D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2822-L2866"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Conv2D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradient of the loss with respect to the layer parameters.</p>
<p class="rubric">Notes</p>
<p>Relies on <a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.im2col" title="numpy_ml.neural_nets.utils.im2col"><code class="xref py py-meth docutils literal notranslate"><span class="pre">im2col()</span></code></a> and
<a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.col2im" title="numpy_ml.neural_nets.utils.col2im"><code class="xref py py-meth docutils literal notranslate"><span class="pre">col2im()</span></code></a> to vectorize the
gradient calculation.</p>
<p>See the private method <code class="xref py py-meth docutils literal notranslate"><span class="pre">_backward_naive()</span></code> for a more straightforward
implementation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <a href="#id1"><span class="problematic" id="id2">`</span></a>(n_ex, out_rows,) – </li>
<li><strong>out_ch</strong><strong>)</strong><strong>`</strong><strong> or </strong><strong>list of arrays</strong> (<em>out_cols</em><em>,</em>) – The gradient(s) of the loss with respect to the layer output(s).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss with respect to the layer input volume.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="deconv2d">
<h2><code class="docutils literal notranslate"><span class="pre">Deconv2D</span></code><a class="headerlink" href="#deconv2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Deconv2D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Deconv2D</code><span class="sig-paren">(</span><em>out_ch</em>, <em>kernel_shape</em>, <em>pad=0</em>, <em>stride=1</em>, <em>act_fn=None</em>, <em>optimizer=None</em>, <em>init='glorot_uniform'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3128-L3343"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Deconv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>Apply a two-dimensional “deconvolution” to an input volume.</p>
<p class="rubric">Notes</p>
<p>The term “deconvolution” in this context does not correspond with the
deconvolution operation in mathematics. More accurately, this layer is
computing a transposed convolution / fractionally-strided convolution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>out_ch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of filters/kernels to compute in the current layer</li>
<li><strong>kernel_shape</strong> (<em>2-tuple</em>) – The dimension of a single 2D filter/kernel in the current layer</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">Y[t]</span></code>. If None, use
<a class="reference internal" href="numpy_ml.neural_nets.activations.html#numpy_ml.neural_nets.activations.Affine" title="numpy_ml.neural_nets.activations.Affine"><code class="xref py py-class docutils literal notranslate"><span class="pre">Affine</span></code></a>
activations by default. Default is None.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, or </em><em>'same'</em>) – The number of rows/columns to zero-pad the input with. Default is 0.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride/hop of the convolution kernels as they move over the
input volume. Default is 1.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Deconv2D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3195-L3211"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Deconv2D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Deconv2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3213-L3253"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Deconv2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output given input volume <cite>X</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The input volume consisting of <cite>n_ex</cite> examples, each with dimension
(<cite>in_rows</cite>, <cite>in_cols</cite>, <cite>in_ch</cite>).</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, out_rows, out_cols, out_ch)</cite>) – The layer output.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Deconv2D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdY</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3255-L3294"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Deconv2D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradient of the loss with respect to the layer parameters.</p>
<p class="rubric">Notes</p>
<p>Relies on <a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.im2col" title="numpy_ml.neural_nets.utils.im2col"><code class="xref py py-meth docutils literal notranslate"><span class="pre">im2col()</span></code></a> and
<a class="reference internal" href="numpy_ml.neural_nets.utils.html#numpy_ml.neural_nets.utils.col2im" title="numpy_ml.neural_nets.utils.col2im"><code class="xref py py-meth docutils literal notranslate"><span class="pre">col2im()</span></code></a> to vectorize the
gradient calculations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>n_ex, out_rows, out_cols, out_ch</cite>)) – The gradient of the loss with respect to the layer output.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>n_ex, in_rows, in_cols, in_ch</cite>)) – The gradient of the loss with respect to the layer input volume.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dotproductattention">
<h2><code class="docutils literal notranslate"><span class="pre">DotProductAttention</span></code><a class="headerlink" href="#dotproductattention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">DotProductAttention</code><span class="sig-paren">(</span><em>scale=True</em>, <em>dropout_p=0</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L136-L335"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single “attention head” layer using a dot-product for the scoring function.</p>
<p class="rubric">Notes</p>
<p>The equations for a dot product attention layer are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{Z}  &amp;=  \mathbf{K Q}^\top \ \ \ \ &amp;&amp;\text{if scale = False} \\
            &amp;=  \mathbf{K Q}^\top / \sqrt{d_k} \ \ \ \ &amp;&amp;\text{if scale = True} \\
\mathbf{Y}  &amp;=  \text{dropout}(\text{softmax}(\mathbf{Z})) \mathbf{V}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to scale the the key-query dot product by the square root
of the key/query vector dimensionality before applying the Softmax.
This is useful, since the scale of dot product will otherwise
increase as query / key dimensions grow. Default is True.</li>
<li><strong>dropout_p</strong> (<em>float in</em><em> [</em><em>0</em><em>, </em><em>1</em><em>)</em>) – The dropout propbability during training, applied to the output of
the softmax. If 0, no dropout is applied. Default is 0.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.
Unused.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None. Unused.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L185-L197"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L199-L201"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze the layer parameters at their current values so they can no
longer be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L203-L205"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze the layer parameters so they can be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>Q</em>, <em>K</em>, <em>V</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L207-L276"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the attention-weighted output of a collection of keys, values,
and queries.</p>
<p class="rubric">Notes</p>
<p>In the most abstract (ie., hand-wave-y) sense:</p>
<blockquote>
<div><ul class="simple">
<li>Query vectors ask questions</li>
<li>Key vectors advertise their relevancy to questions</li>
<li>Value vectors give possible answers to questions</li>
<li>The dot product between Key and Query vectors provides scores for
each of the the <cite>n_ex</cite> different Value vectors</li>
</ul>
</div></blockquote>
<p>For a single query and <cite>n</cite> key-value pairs, dot-product attention (with
scaling) is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>w0 = dropout(softmax( (query @ key[0]) / sqrt(d_k) ))
w1 = dropout(softmax( (query @ key[1]) / sqrt(d_k) ))
                        ...
wn = dropout(softmax( (query @ key[n]) / sqrt(d_k) ))

y = np.array([w0, ..., wn]) @ values
          (1 × n_ex)      (n_ex × d_v)
</pre></div>
</div>
<p>In words, keys and queries are combined via dot-product to produce a
score, which is then passed through a softmax to produce a weight on
each value vector in Values. We elementwise multiply each value vector
by its weight, and then take the elementwise sum of each weighted value
vector to get the <span class="math notranslate nohighlight">\(1 \times d_v\)</span> output for the current example.</p>
<p>In vectorized form,</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \text{dropout}(\text{softmax}( \mathbf{KQ}^\top / \sqrt{d_k})) \mathbf{V}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Q</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_k)</cite>) – A set of <cite>n_ex</cite> query vectors packed into a single matrix.
Optional middle dimensions can be used to specify, e.g., the number
of parallel attention heads.</li>
<li><strong>K</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_k)</cite>) – A set of <cite>n_ex</cite> key vectors packed into a single matrix. Optional
middle dimensions can be used to specify, e.g., the number of
parallel attention heads.</li>
<li><strong>V</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_v)</cite>) – A set of <cite>n_ex</cite> value vectors packed into a single matrix. Optional
middle dimensions can be used to specify, e.g., the number of
parallel attention heads.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_v)</cite>) – The attention-weighted output values</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.DotProductAttention.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L286-L323"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.DotProductAttention.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_v)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite></li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>dQ</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_k)</cite> or list of arrays) – The gradient of the loss wrt. the layer query matrix/matrices <cite>Q</cite>.</li>
<li><strong>dK</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_k)</cite> or list of arrays) – The gradient of the loss wrt. the layer key matrix/matrices <cite>K</cite>.</li>
<li><strong>dV</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *, d_v)</cite> or list of arrays) – The gradient of the loss wrt. the layer value matrix/matrices <cite>V</cite>.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="embedding">
<h2><code class="docutils literal notranslate"><span class="pre">Embedding</span></code><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Embedding">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>n_out</em>, <em>vocab_size</em>, <em>pool=None</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1669-L1851"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>An embedding layer.</p>
<p class="rubric">Notes</p>
<p>Equations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</pre></div>
</div>
<p>NB. This layer must be the first in a neural network as the gradients
do not get passed back through to the inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimensionality of the embeddings</li>
<li><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The total number of items in the vocabulary. All integer indices
are expected to range between 0 and <cite>vocab_size - 1</cite>.</li>
<li><strong>pool</strong> (<em>{'sum'</em><em>, </em><em>'mean'</em><em>, </em><em>None}</em>) – If not None, apply this function to the collection of <cite>n_in</cite>
encodings in each example to produce a single, pooled embedding.
Default is None.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Embedding.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1725-L1738"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Embedding.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Embedding.lookup">
<code class="descname">lookup</code><span class="sig-paren">(</span><em>ids</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1740-L1754"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Embedding.lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the embeddings associated with the IDs in <cite>ids</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>word_ids</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>M</cite>,)) – An array of <cite>M</cite> IDs to retrieve embeddings for.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>embeddings</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>M</cite>, <cite>n_out</cite>)) – The embedding vectors for each of the <cite>M</cite> IDs.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Embedding.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1756-L1796"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Embedding.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<p class="rubric">Notes</p>
<dl class="docutils">
<dt>Equations:</dt>
<dd>Y = W[x]</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite> or list of length <cite>n_ex</cite>) – Layer input, representing a minibatch of <cite>n_ex</cite> examples. If
<code class="docutils literal notranslate"><span class="pre">self.pool</span></code> is None, each example must consist of exactly <cite>n_in</cite>
integer token IDs. Otherwise, <cite>X</cite> can be a ragged array, with each
example consisting of a variable number of token IDs.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through with regard to this input.
Default is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in, n_out)</cite>) – Embeddings for each coordinate of each of the <cite>n_ex</cite> examples</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Embedding.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1809-L1835"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Embedding.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to embedding weights.</p>
<p class="rubric">Notes</p>
<p>Because the items in <cite>X</cite> are interpreted as indices, we cannot compute
the gradient of the layer output wrt. <cite>X</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in, n_out)</cite> or list of arrays) – The gradient(s) of the loss wrt. the layer output(s)</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="flatten">
<h2><code class="docutils literal notranslate"><span class="pre">Flatten</span></code><a class="headerlink" href="#flatten" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Flatten">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Flatten</code><span class="sig-paren">(</span><em>keep_dim='first'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L786-L875"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>Flatten a multidimensional input into a 2D matrix.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>keep_dim</strong> (<em>{'first'</em><em>, </em><em>'last'</em><em>, </em><em>-1}</em>) – The dimension of the original input to retain. Typically used for
retaining the minibatch dimension.. If -1, flatten all dimensions.
Default is ‘first’.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Flatten.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L814-L824"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Flatten.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Flatten.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L826-L851"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Flatten.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a>) – Input volume to flatten.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(*out_dims)</cite>) – Flattened output. If <cite>keep_dim</cite> is <cite>‘first’</cite>, <cite>X</cite> is reshaped to
<code class="docutils literal notranslate"><span class="pre">(X.shape[0],</span> <span class="pre">-1)</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">(-1,</span> <span class="pre">X.shape[0])</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Flatten.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L853-L875"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Flatten.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(*out_dims)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(*in_dims)</cite> or list of arrays) – The gradient of the loss wrt. the layer input(s) <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="fullyconnected">
<h2><code class="docutils literal notranslate"><span class="pre">FullyConnected</span></code><a class="headerlink" href="#fullyconnected" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.FullyConnected">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">FullyConnected</code><span class="sig-paren">(</span><em>n_out</em>, <em>act_fn=None</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1854-L2019"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.FullyConnected" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A fully-connected (dense) layer.</p>
<p class="rubric">Notes</p>
<p>A fully connected layer computes the function</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = f( \mathbf{WX} + \mathbf{b} )\]</div>
<p>where <cite>f</cite> is the activation nonlinearity, <strong>W</strong> and <strong>b</strong> are
parameters of the layer, and <strong>X</strong> is the minibatch of input examples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimensionality of the layer output</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The element-wise output nonlinearity used in computing <cite>Y</cite>. If None,
use the identity function <span class="math notranslate nohighlight">\(f(X) = X\)</span>. Default is None.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.FullyConnected.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1905-L1918"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.FullyConnected.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.FullyConnected.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1920-L1950"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.FullyConnected.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.FullyConnected.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1961-L1993"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.FullyConnected.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite> or list of arrays) – The gradient(s) of the loss wrt. the layer output(s).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dLdX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite> or list of arrays) – The gradient of the loss wrt. the layer input(s) <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="lstm">
<h2><code class="docutils literal notranslate"><span class="pre">LSTM</span></code><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.LSTM">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>n_out</em>, <em>act_fn='Tanh'</em>, <em>gate_fn='Sigmoid'</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3995-L4136"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single long short-term memory (LSTM) RNN layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of a single hidden state / output on a given timestep.</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">A[t]</span></code>. Default is <cite>‘Tanh’</cite>.</li>
<li><strong>gate_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The gate function for computing the update, forget, and output
gates. Default is <cite>‘Sigmoid’</cite>.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <a class="reference internal" href="#numpy_ml.neural_nets.layers.LSTM.update" title="numpy_ml.neural_nets.layers.LSTM.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LSTM.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4044-L4055"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4057-L4082"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a forward pass across all timesteps in the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in, n_t)</cite>) – Input consisting of <cite>n_ex</cite> examples each of dimensionality <cite>n_in</cite>
and extending for <cite>n_t</cite> timesteps.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out, n_t)</cite>) – The value of the hidden state for each of the <cite>n_ex</cite> examples
across each of the <cite>n_t</cite> timesteps.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdA</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4084-L4107"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a backward pass across all timesteps in the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dLdA</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out, n_t)</cite>) – The gradient of the loss with respect to the layer output for each
of the <cite>n_ex</cite> examples across all <cite>n_t</cite> timesteps.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>dLdX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>n_ex</cite>, <cite>n_in</cite>, <cite>n_t</cite>)) – The value of the hidden state for each of the <cite>n_ex</cite> examples
across each of the <cite>n_t</cite> timesteps.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LSTM.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4109-L4111"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LSTM.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4113-L4115"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LSTM.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4117-L4119"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4121-L4122"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze the layer parameters at their current values so they can no
longer be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4124-L4125"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze the layer parameters so they can be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>summary_dict</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4127-L4129"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the layer parameters from a dictionary of values.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>summary_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – A dictionary of layer parameters and hyperparameters. If a required
parameter or hyperparameter is not included within <cite>summary_dict</cite>,
this method will use the value in the current layer’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">summary()</span></code> method.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>layer</strong> (<a class="reference internal" href="#"><span class="doc">Layer</span></a> object) – The newly-initialized layer.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4131-L4132"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Erase all the layer’s derived variables and gradients.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTM.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L4134-L4136"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTM.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the layer parameters using the accrued gradients and layer
optimizer. Flush all gradients once the update is complete.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="lstmcell">
<h2><code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.LSTMCell">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>n_out</em>, <em>act_fn='Tanh'</em>, <em>gate_fn='Sigmoid'</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3556-L3858"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single step of a long short-term memory (LSTM) RNN.</p>
<p class="rubric">Notes</p>
<p>Notation:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">Z[t]</span></code>  is the input to each of the gates at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">A[t]</span></code>  is the value of the hidden state at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">Cc[t]</span></code> is the value of the <em>candidate</em> cell/memory state at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">C[t]</span></code>  is the value of the <em>final</em> cell/memory state at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">Gf[t]</span></code> is the output of the forget gate at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">Gu[t]</span></code> is the output of the update gate at timestep <cite>t</cite></li>
<li><code class="docutils literal notranslate"><span class="pre">Go[t]</span></code> is the output of the output gate at timestep <cite>t</cite></li>
</ul>
<p>Equations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">stack</span><span class="p">([</span><span class="n">A</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">]])</span>
<span class="n">Gf</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">gate_fn</span><span class="p">(</span><span class="n">Wf</span> <span class="o">@</span> <span class="n">Z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">bf</span><span class="p">)</span>
<span class="n">Gu</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">gate_fn</span><span class="p">(</span><span class="n">Wu</span> <span class="o">@</span> <span class="n">Z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">bu</span><span class="p">)</span>
<span class="n">Go</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">gate_fn</span><span class="p">(</span><span class="n">Wo</span> <span class="o">@</span> <span class="n">Z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">bo</span><span class="p">)</span>
<span class="n">Cc</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">act_fn</span><span class="p">(</span><span class="n">Wc</span> <span class="o">@</span> <span class="n">Z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">bc</span><span class="p">)</span>
<span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">Gf</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">Gu</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">Cc</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
<span class="n">A</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">Go</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">act_fn</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
</pre></div>
</div>
<p>where <cite>&#64;</cite> indicates dot/matrix product, and ‘*’ indicates elementwise
multiplication.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of a single hidden state / output on a given timestep.</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">A[t]</span></code>. Default is
<cite>‘Tanh’</cite>.</li>
<li><strong>gate_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The gate function for computing the update, forget, and output
gates. Default is <cite>‘Sigmoid’</cite>.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with default
parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LSTMCell.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3695-L3709"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTMCell.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTMCell.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>Xt</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3711-L3766"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTMCell.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output for a single timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Input at timestep t consisting of <cite>n_ex</cite> examples each of
dimensionality <cite>n_in</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>At</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – The value of the hidden state at timestep <cite>t</cite> for each of the <cite>n_ex</cite>
examples.</li>
<li><strong>Ct</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – The value of the cell/memory state at timestep <cite>t</cite> for each of the
<cite>n_ex</cite> examples.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTMCell.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdAt</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3768-L3844"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTMCell.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop for a single timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dLdAt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – The gradient of the loss wrt. the layer outputs (ie., hidden
states) at timestep <cite>t</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>dLdXt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer inputs at timestep <cite>t</cite>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LSTMCell.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3846-L3858"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LSTMCell.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Erase all the layer’s derived variables and gradients.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="layernorm1d">
<h2><code class="docutils literal notranslate"><span class="pre">LayerNorm1D</span></code><a class="headerlink" href="#layernorm1d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.LayerNorm1D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">LayerNorm1D</code><span class="sig-paren">(</span><em>epsilon=1e-05</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1506-L1661"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A layer normalization layer for 1D inputs.</p>
<p class="rubric">Notes</p>
<p>In contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.BatchNorm1D" title="numpy_ml.neural_nets.layers.BatchNorm1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1D</span></code></a>, the LayerNorm layer calculates the
mean and variance across <em>features</em> rather than examples in the batch
ensuring that the mean and variance estimates are independent of batch
size and permitting straightforward application in RNNs.</p>
<p>Equations [train &amp; test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Also in contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.BatchNorm1D" title="numpy_ml.neural_nets.layers.BatchNorm1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1D</span></code></a>, <cite>scaler</cite> and <cite>intercept</cite> are applied
<em>elementwise</em> to <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small smoothing constant to use during computation of <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>
to avoid divide-by-zero errors. Default is 1e-5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LayerNorm1D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1557-L1570"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm1D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LayerNorm1D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1572-L1606"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm1D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LayerNorm1D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1608-L1640"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm1D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="layernorm2d">
<h2><code class="docutils literal notranslate"><span class="pre">LayerNorm2D</span></code><a class="headerlink" href="#layernorm2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.LayerNorm2D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">LayerNorm2D</code><span class="sig-paren">(</span><em>epsilon=1e-05</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1330-L1503"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A layer normalization layer for 2D inputs with an additional channel
dimension.</p>
<p class="rubric">Notes</p>
<p>In contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.BatchNorm2D" title="numpy_ml.neural_nets.layers.BatchNorm2D"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2D</span></code></a>, the LayerNorm layer calculates the
mean and variance across <em>features</em> rather than examples in the batch
ensuring that the mean and variance estimates are independent of batch
size and permitting straightforward application in RNNs.</p>
<p>Equations [train &amp; test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Also in contrast to <a class="reference internal" href="#numpy_ml.neural_nets.layers.BatchNorm2D" title="numpy_ml.neural_nets.layers.BatchNorm2D"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2D</span></code></a>, <cite>scaler</cite> and <cite>intercept</cite> are applied
<em>elementwise</em> to <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small smoothing constant to use during computation of <code class="docutils literal notranslate"><span class="pre">norm(X)</span></code>
to avoid divide-by-zero errors. Default is 1e-5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.LayerNorm2D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1385-L1398"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm2D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LayerNorm2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1400-L1442"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<p class="rubric">Notes</p>
<p>Equations [train &amp; test]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">scaler</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>
<span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – Input volume containing the <cite>in_rows</cite> by <cite>in_cols</cite>-dimensional
features for a minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.LayerNorm2D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L1444-L1476"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.LayerNorm2D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multiply">
<h2><code class="docutils literal notranslate"><span class="pre">Multiply</span></code><a class="headerlink" href="#multiply" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Multiply">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Multiply</code><span class="sig-paren">(</span><em>act_fn=None</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L688-L783"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A multiplication layer that returns the <em>elementwise</em> product of its
inputs, passed through an optional nonlinearity.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The element-wise output nonlinearity used in computing the final
output. If None, use the identity function <span class="math notranslate nohighlight">\(f(x) = x\)</span>.
Default is None.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Multiply.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L713-L723"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Multiply.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Multiply.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L725-L750"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Multiply.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (list of length <cite>n_inputs</cite>) – A list of tensors, all of the same shape.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *)</cite>) – The product over the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Multiply.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdY</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L752-L776"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Multiply.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, *)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (list of length <cite>n_inputs</cite>) – The gradient of the loss wrt. each input in <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="pool2d">
<h2><code class="docutils literal notranslate"><span class="pre">Pool2D</span></code><a class="headerlink" href="#pool2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Pool2D">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Pool2D</code><span class="sig-paren">(</span><em>kernel_shape</em>, <em>stride=1</em>, <em>pad=0</em>, <em>mode='max'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2956-L3125"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Pool2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single two-dimensional pooling layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_shape</strong> (<em>2-tuple</em>) – The dimension of a single 2D filter/kernel in the current layer</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The stride/hop of the convolution kernels as they move over the
input volume. Default is 1.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, or </em><em>'same'</em>) – The number of rows/columns of 0’s to pad the input. Default is 0.</li>
<li><strong>mode</strong> (<em>{&quot;max&quot;</em><em>, </em><em>&quot;average&quot;}</em>) – The pooling function to apply.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Pool2D.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2992-L3008"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Pool2D.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Pool2D.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3010-L3063"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Pool2D.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output given input volume <cite>X</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The input volume consisting of <cite>n_ex</cite> examples, each with dimension
(<cite>in_rows</cite>,`in_cols`, <cite>in_ch</cite>)</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, out_rows, out_cols, out_ch)</cite>) – The layer output.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Pool2D.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdY</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3065-L3125"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Pool2D.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdY</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer output <cite>Y</cite>.</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_rows, in_cols, in_ch)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="rnn">
<h2><code class="docutils literal notranslate"><span class="pre">RNN</span></code><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.RNN">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>n_out</em>, <em>act_fn='Tanh'</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3861-L3992"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single vanilla (Elman)-RNN layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of a single hidden state / output on a given
timestep.</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">A[t]</span></code>. Default is
<cite>‘Tanh’</cite>.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <a class="reference internal" href="#numpy_ml.neural_nets.layers.RNN.update" title="numpy_ml.neural_nets.layers.RNN.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with default
parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RNN.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3901-L3911"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3913-L3938"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a forward pass across all timesteps in the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in, n_t)</cite>) – Input consisting of <cite>n_ex</cite> examples each of dimensionality <cite>n_in</cite>
and extending for <cite>n_t</cite> timesteps.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out, n_t)</cite>) – The value of the hidden state for each of the <cite>n_ex</cite> examples
across each of the <cite>n_t</cite> timesteps.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdA</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3940-L3963"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a backward pass across all timesteps in the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dLdA</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out, n_t)</cite>) – The gradient of the loss with respect to the layer output for each
of the <cite>n_ex</cite> examples across all <cite>n_t</cite> timesteps.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>dLdX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in, n_t)</cite>) – The value of the hidden state for each of the <cite>n_ex</cite> examples
across each of the <cite>n_t</cite> timesteps.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RNN.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3965-L3967"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RNN.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3969-L3971"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RNN.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3973-L3975"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.set_params">
<code class="descname">set_params</code><span class="sig-paren">(</span><em>summary_dict</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3977-L3979"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the layer parameters from a dictionary of values.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>summary_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – A dictionary of layer parameters and hyperparameters. If a required
parameter or hyperparameter is not included within <cite>summary_dict</cite>,
this method will use the value in the current layer’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">summary()</span></code> method.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>layer</strong> (<a class="reference internal" href="#"><span class="doc">Layer</span></a> object) – The newly-initialized layer.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3981-L3982"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze the layer parameters at their current values so they can no
longer be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3984-L3985"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze the layer parameters so they can be updated.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3987-L3988"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Erase all the layer’s derived variables and gradients.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNN.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3990-L3992"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNN.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the layer parameters using the accrued gradients and layer
optimizer. Flush all gradients once the update is complete.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="rnncell">
<h2><code class="docutils literal notranslate"><span class="pre">RNNCell</span></code><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.RNNCell">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>n_out</em>, <em>act_fn='Tanh'</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3351-L3553"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A single step of a vanilla (Elman) RNN.</p>
<p class="rubric">Notes</p>
<p>At timestep <cite>t</cite>, the vanilla RNN cell computes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{Z}^{(t)}  &amp;=
    \mathbf{W}_{ax} \mathbf{X}^{(t)} + \mathbf{b}_{ax} +
        \mathbf{W}_{aa} \mathbf{A}^{(t-1)} + \mathbf{b}_{aa} \\
\mathbf{A}^{(t)}  &amp;=  f(\mathbf{Z}^{(t)})\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\mathbf{X}^{(t)}\)</span> is the input at time <cite>t</cite></li>
<li><span class="math notranslate nohighlight">\(\mathbf{A}^{(t)}\)</span> is the hidden state at timestep <cite>t</cite></li>
<li><cite>f</cite> is the layer activation function</li>
<li><span class="math notranslate nohighlight">\(\mathbf{W}_{ax}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_{ax}\)</span> are the weights
and bias for the input to hidden layer</li>
<li><span class="math notranslate nohighlight">\(\mathbf{W}_{aa}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_{aa}\)</span> are the weights
and biases for the hidden to hidden layer</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of a single hidden state / output on a given timestep</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The activation function for computing <code class="docutils literal notranslate"><span class="pre">A[t]</span></code>. Default is <cite>‘Tanh’</cite>.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with default
parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RNNCell.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3429-L3442"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNNCell.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNNCell.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>Xt</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3444-L3490"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNNCell.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the network output for a single timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Input at timestep <cite>t</cite> consisting of <cite>n_ex</cite> examples each of
dimensionality <cite>n_in</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>At</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – The value of the hidden state at timestep <cite>t</cite> for each of the
<cite>n_ex</cite> examples.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNNCell.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdAt</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3492-L3539"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNNCell.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop for a single timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dLdAt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – The gradient of the loss wrt. the layer outputs (ie., hidden
states) at timestep <cite>t</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>dLdXt</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer inputs at timestep <cite>t</cite>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RNNCell.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L3541-L3553"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RNNCell.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Erase all the layer’s derived variables and gradients.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="rbm">
<h2><code class="docutils literal notranslate"><span class="pre">RBM</span></code><a class="headerlink" href="#rbm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.RBM">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">RBM</code><span class="sig-paren">(</span><em>n_out</em>, <em>K=1</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L338-L584"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A Restricted Boltzmann machine with Bernoulli visible and hidden units.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output dimensions/units.</li>
<li><strong>K</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of contrastive divergence steps to run before computing
a single gradient update. Default is 1.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.RBM.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L396-L409"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RBM.CD_update">
<code class="descname">CD_update</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L411-L425"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM.CD_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a single contrastive divergence-<cite>k</cite> training update using the
visible inputs <cite>X</cite> as a starting point for the Gibbs sampler.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples. Each feature in X should ideally be
binary-valued, although it is possible to also train on real-valued
features ranging between (0, 1) (e.g., grayscale images).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RBM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>V</em>, <em>K=None</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L427-L520"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform the CD-<cite>k</cite> “forward pass” of visible inputs into hidden units
and back.</p>
<p class="rubric">Notes</p>
<p>This implementation follows <a class="footnote-reference" href="#id4" id="id3">[1]</a>’s recommendations for the RBM forward
pass:</p>
<blockquote>
<div><ul class="simple">
<li>Use real-valued probabilities for both the data and the visible
unit reconstructions.</li>
<li>Only the final update of the hidden units should use the actual
probabilities – all others should be sampled binary states.</li>
<li>When collecting the pairwise statistics for learning weights or
the individual statistics for learning biases, use the
probabilities, not the binary states.</li>
</ul>
</div></blockquote>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[1]</a></td><td>Hinton, G. (2010). “A practical guide to training restricted
Boltzmann machines”. <em>UTML TR 2010-003</em></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>V</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Visible input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples. Each feature in V should ideally be
binary-valued, although it is possible to also train on real-valued
features ranging between (0, 1) (e.g., grayscale images).</li>
<li><strong>K</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of steps of contrastive divergence steps to run before
computing the gradient update. If None, use <code class="docutils literal notranslate"><span class="pre">self.K</span></code>. Default is
None.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RBM.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>retain_grads=True</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L522-L544"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a gradient update on the layer parameters via the contrastive
divergence equations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.RBM.reconstruct">
<code class="descname">reconstruct</code><span class="sig-paren">(</span><em>X</em>, <em>n_steps=10</em>, <em>return_prob=False</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L546-L584"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.RBM.reconstruct" title="Permalink to this definition">¶</a></dt>
<dd><p>Reconstruct an input <cite>X</cite> by running the trained Gibbs sampler for
<cite>n_steps</cite>-worth of CD-<cite>k</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples. Each feature in <cite>X</cite> should ideally be
binary-valued, although it is possible to also train on real-valued
features ranging between (0, 1) (e.g., grayscale images). If <cite>X</cite> has
missing values, it may be sufficient to mark them with random
entries and allow the reconstruction to impute them.</li>
<li><strong>n_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of Gibbs sampling steps to perform when generating the
reconstruction. Default is 10.</li>
<li><strong>return_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to return the real-valued feature probabilities for the
reconstruction or the binary samples. Default is False.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>V</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, in_ch)</cite>) – The reconstruction (or feature probabilities if <cite>return_prob</cite> is
true) of the visual input <cite>X</cite> after running the Gibbs sampler for
<cite>n_steps</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="softmax">
<h2><code class="docutils literal notranslate"><span class="pre">Softmax</span></code><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.Softmax">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">Softmax</code><span class="sig-paren">(</span><em>dim=-1</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2022-L2165"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A softmax nonlinearity layer.</p>
<p class="rubric">Notes</p>
<p>This is implemented as a layer rather than an activation primarily
because it requires retaining the layer input in order to compute the
softmax gradients properly. In other words, in contrast to other
simple activations, the softmax function and its gradient are not
computed elementwise, and thus are more easily expressed as a layer.</p>
<p>The softmax function computes:</p>
<div class="math notranslate nohighlight">
\[y_i = \frac{e^{x_i}}{\sum_j e^{x_j}}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is the <cite>i</cite> th element of input example <strong>x</strong>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension in <cite>X</cite> along which the softmax will be computed.
Default is -1.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None. Unused for this layer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.Softmax.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2066-L2077"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Softmax.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Softmax.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2079-L2108"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Softmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.Softmax.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2116-L2144"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.Softmax.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite> or list of arrays) – The gradient(s) of the loss wrt. the layer output(s).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dLdX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sparseevolution">
<h2><code class="docutils literal notranslate"><span class="pre">SparseEvolution</span></code><a class="headerlink" href="#sparseevolution" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="numpy_ml.neural_nets.layers.SparseEvolution">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.layers.</code><code class="descname">SparseEvolution</code><span class="sig-paren">(</span><em>n_out</em>, <em>zeta=0.3</em>, <em>epsilon=20</em>, <em>act_fn=None</em>, <em>init='glorot_uniform'</em>, <em>optimizer=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2168-L2397"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.SparseEvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#numpy_ml.neural_nets.layers.layers.LayerBase" title="numpy_ml.neural_nets.layers.layers.LayerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy_ml.neural_nets.layers.layers.LayerBase</span></code></a></p>
<p>A sparse Erdos-Renyi layer with evolutionary rewiring via the sparse
evolutionary training (SET) algorithm.</p>
<p class="rubric">Notes</p>
<div class="math notranslate nohighlight">
\[Y = f( (\mathbf{W} \odot \mathbf{W}_{mask}) \mathbf{X} + \mathbf{b} )\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> is the elementwise multiplication operation, <cite>f</cite> is
the layer activation function, and <span class="math notranslate nohighlight">\(\mathbf{W}_{mask}\)</span> is an
evolved binary mask.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_out</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimensionality of the layer output</li>
<li><strong>zeta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Proportion of the positive and negative weights closest to zero to
drop after each training update. Default is 0.3.</li>
<li><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Layer sparsity parameter. Default is 20.</li>
<li><strong>act_fn</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.activations.html"><span class="doc">Activation</span></a> object, or None) – The element-wise output nonlinearity used in computing <cite>Y</cite>. If None,
use the identity function <span class="math notranslate nohighlight">\(f(X) = X\)</span>. Default is None.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is <cite>‘glorot_uniform’</cite>.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <a class="reference internal" href="#numpy_ml.neural_nets.layers.SparseEvolution.update" title="numpy_ml.neural_nets.layers.SparseEvolution.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a> method.  If None, use the <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with default
parameters. Default is None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.layers.SparseEvolution.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2239-L2254"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.SparseEvolution.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the layer hyperparameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.SparseEvolution.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2256-L2286"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.SparseEvolution.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the layer output on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing the <cite>n_in</cite>-dimensional features for a
minibatch of <cite>n_ex</cite> examples.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If False, this suggests the layer
will not be expected to backprop through wrt. this input. Default
is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>Y</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite>) – Layer output for each of the <cite>n_ex</cite> examples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.SparseEvolution.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>dLdy</em>, <em>retain_grads=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2298-L2330"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.SparseEvolution.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backprop from layer outputs to inputs</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dLdy</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_out)</cite> or list of arrays) – The gradient(s) of the loss wrt. the layer output(s).</li>
<li><strong>retain_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to include the intermediate parameter gradients computed
during the backward pass in the final parameter update. Default is
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>dLdX</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – The gradient of the loss wrt. the layer input <cite>X</cite>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.layers.SparseEvolution.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/layers/layers.py#L2360-L2370"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.layers.SparseEvolution.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters using current gradients and evolve network
connections via SET.</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2019, David Bourgin.
      
      |
      <a href="_sources/numpy_ml.neural_nets.layers.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-65839510-3']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>