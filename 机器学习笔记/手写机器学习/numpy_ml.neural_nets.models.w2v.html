
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Word2Vec (neural_nets.models.w2v) &#8212; numpy-ml 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">numpy-ml</a></h1>



<p class="blurb">Machine learning, in NumPy</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=ddbourgin&repo=numpy-ml&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.hmm.html">Hidden Markov models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.gmm.html">Gaussian mixture models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.lda.html">Latent Dirichlet allocation</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.ngram.html">N-gram smoothing models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.rl_models.html">Reinforcement learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.nonparametric.html">Nonparametric models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.trees.html">Tree-based models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.neural_nets.html">Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.linear_models.html">Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_ml.utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-numpy_ml.neural_nets.models.w2v">
<span id="word2vec-neural-nets-models-w2v"></span><h1>Word2Vec (<code class="docutils literal notranslate"><span class="pre">neural_nets.models.w2v</span></code>)<a class="headerlink" href="#module-numpy_ml.neural_nets.models.w2v" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec">
<em class="property">class </em><code class="descclassname">numpy_ml.neural_nets.models.w2v.</code><code class="descname">Word2Vec</code><span class="sig-paren">(</span><em>context_len=5</em>, <em>min_count=None</em>, <em>skip_gram=False</em>, <em>max_tokens=None</em>, <em>embedding_dim=300</em>, <em>filter_stopwords=True</em>, <em>noise_dist_power=0.75</em>, <em>init='glorot_uniform'</em>, <em>num_negative_samples=64</em>, <em>optimizer='SGD(lr=0.1)'</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L12-L451"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>A word2vec model supporting both continuous bag of words (CBOW) and
skip-gram architectures, with training via noise contrastive
estimation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of words to the left and right of the current word to
use as context during training. Larger values result in more
training examples and thus can lead to higher accuracy at the
expense of additional training time. Default is 5.</li>
<li><strong>min_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.8)"><em>None</em></a>) – Minimum number of times a token must occur in order to be included
in vocab. If None, include all tokens from <cite>corpus_fp</cite> in vocab.
Default is None.</li>
<li><strong>skip_gram</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to train the skip-gram or CBOW model. The skip-gram model
is trained to predict the target word i given its surrounding
context, <code class="docutils literal notranslate"><span class="pre">words[i</span> <span class="pre">-</span> <span class="pre">context:i]</span></code> and <code class="docutils literal notranslate"><span class="pre">words[i</span> <span class="pre">+</span> <span class="pre">1:i</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">+</span>
<span class="pre">context]</span></code> as input. Default is False.</li>
<li><strong>max_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.8)"><em>None</em></a>) – Only add the first <cite>max_tokens</cite> most frequent tokens that occur
more than <cite>min_count</cite> to the vocabulary.  If None, add all tokens
that occur more than than <cite>min_count</cite>. Default is None.</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of dimensions in the final word embeddings. Default is
300.</li>
<li><strong>filter_stopwords</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to remove stopwords before encoding the words in the
corpus. Default is True.</li>
<li><strong>noise_dist_power</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The power the unigram count is raised to when computing the noise
distribution for negative sampling. A value of 0 corresponds to a
uniform distribution over tokens, and a value of 1 corresponds to a
distribution proportional to the token unigram counts. Default is
0.75.</li>
<li><strong>init</strong> (<em>{'glorot_normal'</em><em>, </em><em>'glorot_uniform'</em><em>, </em><em>'he_normal'</em><em>, </em><em>'he_uniform'}</em>) – The weight initialization strategy. Default is ‘glorot_uniform’.</li>
<li><strong>num_negative_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of negative samples to draw from the noise distribution
for each positive training sample. If 0, use the hierarchical
softmax formulation of the model instead. Default is 5.</li>
<li><strong>optimizer</strong> (str, <a class="reference internal" href="numpy_ml.neural_nets.optimizers.html"><span class="doc">Optimizer</span></a> object, or None) – The optimization strategy to use when performing gradient updates
within the <cite>update</cite> method.  If None, use the
<a class="reference internal" href="numpy_ml.neural_nets.optimizers.html#numpy_ml.neural_nets.optimizers.SGD" title="numpy_ml.neural_nets.optimizers.SGD"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGD</span></code></a> optimizer with
default parameters. Default is None.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.parameters" title="numpy_ml.neural_nets.layers.LSTM.parameters"><strong>parameters</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.Add.hyperparameters" title="numpy_ml.neural_nets.layers.Add.hyperparameters"><strong>hyperparameters</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.derived_variables" title="numpy_ml.neural_nets.layers.LSTM.derived_variables"><strong>derived_variables</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
<li><a class="reference internal" href="numpy_ml.neural_nets.layers.html#numpy_ml.neural_nets.layers.LSTM.gradients" title="numpy_ml.neural_nets.layers.LSTM.gradients"><strong>gradients</strong></a> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The word2vec model is outlined in in [1].</p>
<p>CBOW architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="p">}</span>   <span class="o">----|</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span> <span class="o">----|</span>
<span class="o">...</span>            <span class="o">--&gt;</span> <span class="n">Average</span> <span class="o">--&gt;</span> <span class="n">Embedding</span> <span class="n">layer</span> <span class="o">--&gt;</span> <span class="p">[</span><span class="n">NCE</span> <span class="n">Layer</span> <span class="o">/</span> <span class="n">HSoftmax</span><span class="p">]</span> <span class="o">--&gt;</span> <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="o">...</span><span class="p">})</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">----|</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="p">}</span>   <span class="o">----|</span>
</pre></div>
</div>
<p>Skip-gram architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="n">R</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
<span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">--&gt;</span> <span class="n">Embedding</span> <span class="n">layer</span> <span class="o">--&gt;</span> <span class="p">[</span><span class="n">NCE</span> <span class="n">Layer</span> <span class="o">/</span> <span class="n">HSoftmax</span><span class="p">]</span> <span class="o">--|</span>     <span class="o">...</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
                                                       <span class="o">|--&gt;</span>  <span class="n">P</span><span class="p">(</span><span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="n">R</span><span class="p">}</span> <span class="o">|</span> <span class="n">w_</span><span class="p">{</span><span class="n">t</span><span class="p">})</span>
</pre></div>
</div>
<p>where <span class="math notranslate nohighlight">\(w_{i}\)</span> is the one-hot representation of the word at position
<cite>i</cite> within a sentence in the corpus and <cite>R</cite> is the length of the context
window on either side of the target word.</p>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Mikolov et al. (2013). “Distributed representations of words
and phrases and their compositionality,” Proceedings of the 26th
International Conference on Neural Information Processing Systems.
<a class="reference external" href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a></td></tr>
</tbody>
</table>
<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.parameters">
<code class="descname">parameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L146-L155"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Model parameters</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.hyperparameters">
<code class="descname">hyperparameters</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L157-L180"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Model hyperparameters</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.derived_variables">
<code class="descname">derived_variables</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L182-L193"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.derived_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Variables computed during model operation</p>
</dd></dl>

<dl class="attribute">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.gradients">
<code class="descname">gradients</code><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L195-L204"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Model parameter gradients</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>X</em>, <em>targets</em>, <em>retain_derived=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L206-L233"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the network on a single minibatch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex, n_in)</cite>) – Layer input, representing a minibatch of <cite>n_ex</cite> examples, each
consisting of <cite>n_in</cite> integer word indices</li>
<li><strong>targets</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex,)</cite>) – Target word index for each example in the minibatch.</li>
<li><strong>retain_derived</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to retain the variables calculated during the forward pass
for use later during backprop. If <cite>False</cite>, this suggests the layer
will not be expected to backprop through wrt. this input. Default
True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>loss</strong> (<em>float</em>) – The loss associated with the current minibatch</li>
<li><strong>y_pred</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(n_ex,)</cite>) – The conditional probabilities of the words in <cite>targets</cite> given the
corresponding example / context in <cite>X</cite>.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L235-L240"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the gradient of the loss wrt the current network parameters.</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>cur_loss=None</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L242-L246"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient updates</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.flush_gradients">
<code class="descname">flush_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L248-L251"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.flush_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset parameter gradients after update</p>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.get_embedding">
<code class="descname">get_embedding</code><span class="sig-paren">(</span><em>word_ids</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L253-L269"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.get_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the embeddings for a collection of word IDs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>word_ids</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(M,)</cite>) – An array of word IDs to retrieve embeddings for.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>embeddings</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape <cite>(M, n_out)</cite>) – The embedding vectors for each of the <cite>M</cite> word IDs.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.minibatcher">
<code class="descname">minibatcher</code><span class="sig-paren">(</span><em>corpus_fps</em>, <em>encoding</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L314-L397"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.minibatcher" title="Permalink to this definition">¶</a></dt>
<dd><p>A minibatch generator for skip-gram and CBOW models.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>corpus_fps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><em>list of strs</em>) – The filepath / list of filepaths to the document(s) to be encoded.
Each document is expected to be encoded as newline-separated
string of text, with adjacent tokens separated by a whitespace
character.</li>
<li><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the text encoding for corpus. This value is passed
directly to Python’s <cite>open</cite> builtin. Common entries are either
‘utf-8’ (no header byte), or ‘utf-8-sig’ (header byte).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> (list of length <cite>batchsize</cite> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, <cite>n_in</cite>)) – The context IDs for a minibatch of <cite>batchsize</cite> examples. If
<code class="docutils literal notranslate"><span class="pre">self.skip_gram</span></code> is False, <cite>X</cite> will be a ragged list consisting
of <cite>batchsize</cite> variable-length lists. If <code class="docutils literal notranslate"><span class="pre">self.skip_gram</span></code> is
<cite>True</cite>, all sublists will be of the same length (<cite>n_in</cite>) and <cite>X</cite>
will be returned as a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, <cite>n_in</cite>).</li>
<li><strong>target</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></a> of shape (<cite>batchsize</cite>, 1)) – The target IDs associated with each example in <cite>X</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="numpy_ml.neural_nets.models.w2v.Word2Vec.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>corpus_fps</em>, <em>encoding='utf-8-sig'</em>, <em>n_epochs=20</em>, <em>batchsize=128</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/neural_nets/models/w2v.py#L399-L451"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#numpy_ml.neural_nets.models.w2v.Word2Vec.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn word2vec embeddings for the examples in <cite>X_train</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>corpus_fps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><em>list of strs</em>) – The filepath / list of filepaths to the document(s) to be encoded.
Each document is expected to be encoded as newline-separated
string of text, with adjacent tokens separated by a whitespace
character.</li>
<li><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the text encoding for corpus. Common entries are either
‘utf-8’ (no header byte), or ‘utf-8-sig’ (header byte).  Default
value is ‘utf-8-sig’.</li>
<li><strong>n_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The maximum number of training epochs to run. Default is 20.</li>
<li><strong>batchsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The desired number of examples in each training batch. Default is
128.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Print batch information during training. Default is True.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2019, David Bourgin.
      
      |
      <a href="_sources/numpy_ml.neural_nets.models.w2v.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-65839510-3']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>